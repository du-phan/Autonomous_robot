{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import time\n",
    "import datetime \n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import collections\n",
    "import unicodedata\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime, date, timedelta\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 10,8\n",
    "\n",
    "plt.style.use('bmh')\n",
    "colors = ['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', \n",
    "          '#CC79A7', '#56B4E9', '#009E73', '#F0E442', '#0072B2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_table = [\n",
    "    [ -250 , -15000, -405 , -15000],\n",
    "    [-309 , -15000, -400 , 405  ],\n",
    "    [-262 , -15000, -255 , 400  ],\n",
    "    [-231 , -15000, -77  , 255  ],\n",
    "    [-61  , -15000, 0    , 77   ],\n",
    "    [0    , -15000, 0    , 0    ],\n",
    "    [0    , -15000, -15000, 0    ],\n",
    "    [-325 , 250  , -452 , -15000],\n",
    "    [-270 , 309  , -325 , 452  ],\n",
    "    [-200 , 262  , -190 , 325  ],\n",
    "    [-125 , 231  , -10  , 190  ],\n",
    "    [-2   , 61   , 0    , 10   ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , -15000, 0    ],\n",
    "    [-192 , 325  , -390 , -15000],\n",
    "    [-169 , 270  , -285 , -390 ],\n",
    "    [-105 , 200  , -132 , 285  ],\n",
    "    [-10  , 125  , -5   , 132  ],\n",
    "    [0    , 2    , 0    , 5    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , -15000, 0    ],\n",
    "    [-117 , 192  , -350 , -15000],\n",
    "    [-67  , 169  , -235 , 350  ],\n",
    "    [-8   , 105  , -26  , 235  ],\n",
    "    [0    , 10   , 0    , 26   ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , -15000, 0    ],\n",
    "    [-38  , 117  , -250 , -15000],\n",
    "    [0    , 67   , -148 , 250  ],\n",
    "    [0    , 8    , -3   , 148  ],\n",
    "    [0    , 0    , 0    , 3    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , -15000, 0    ],\n",
    "    [0    , 38   , -195 , -15000],\n",
    "    [0    , 0    , -193 , 195  ],\n",
    "    [0    , 0    , -5   , 193  ],\n",
    "    [0    , 0    , 0    , 5    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , 0    , 0    ],\n",
    "    [0    , 0    , -15000, 0    ],\n",
    "    [-15000, 0    , -255 , -15000],\n",
    "    [-15000, 0    , -190 , 255  ],\n",
    "    [-15000, 0    , -8   , 190  ],\n",
    "    [-15000, 0    , 0    , 8   ],\n",
    "    [-15000, 0    , 0    , 0    ],\n",
    "    [-15000, 0    , 0    , 0    ],\n",
    "    [-15000, 0    , -15000, 0    ]\n",
    "]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(f):\n",
    "    f = f - np.max(f)\n",
    "    p = np.exp(f) / np.sum(np.exp(f))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "softmax_reward_table = np.apply_along_axis(softmax, 1, reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action_is_allowed(learner, state, action):\n",
    "\n",
    "    if (action == 0 and not(state > learner.num_states - learner.servo_num_states - 1)):\n",
    "        return True\n",
    "    elif (action == 1 and not(state < learner.servo_num_states)):\n",
    "        return True\n",
    "    elif (action == 2 and not((state%learner.servo_num_states) == (learner.servo_num_states-1))):\n",
    "        return True\n",
    "    elif (action == 3 and not(state%learner.servo_num_states==0)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearner(object):\n",
    "    def __init__(self, \n",
    "                 servo_num_states, \n",
    "                 num_actions, \n",
    "                 alpha, \n",
    "                 gamma, \n",
    "                 random_action_rate,\n",
    "                 random_action_decay_rate, \n",
    "                 warm_up_period, \n",
    "                 action_penalty,\n",
    "                 negative_reward_coef,\n",
    "                 interest_zone_reward_coef,\n",
    "                 initial_state,\n",
    "                 scaling_point, \n",
    "                 scaling_factor):\n",
    "        \n",
    "        self.servo_num_states = servo_num_states\n",
    "        self.num_states = servo_num_states**2\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.random_action_rate = random_action_rate\n",
    "        self.random_action_decay_rate = random_action_decay_rate\n",
    "        self.warm_up_period = warm_up_period\n",
    "        self.state = initial_state\n",
    "        self.action = 0\n",
    "        self.action_penalty = action_penalty\n",
    "        self.qtable = np.zeros((self.num_states, self.num_actions))\n",
    "        self.num_iteration = 0\n",
    "        self.last_reward = 0\n",
    "        self.negative_reward_coef = negative_reward_coef\n",
    "        self.interest_zone_reward_coef = interest_zone_reward_coef\n",
    "        self.scaling_point = scaling_point\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "    def set_initial_state(self, action):\n",
    "        \"\"\"\n",
    "        @summary: Sets the initial state and returns an action\n",
    "        @param state: The initial state\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        self.state = int(self.num_states/2)\n",
    "        self.action = action #self.qtable[state].argsort()[-1]\n",
    "        \n",
    "    def get_next_state(self):\n",
    "        \n",
    "        next_state = None\n",
    "        \n",
    "        if (self.action == 0 and action_is_allowed(self, self.state, self.action)): \n",
    "            next_state = self.state + self.servo_num_states\n",
    "        elif (self.action == 1 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - self.servo_num_states\n",
    "        elif (self.action == 2 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state + 1\n",
    "        elif (self.action == 3 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - 1 \n",
    "        else:\n",
    "            next_state = self.state;\n",
    "            \n",
    "        return next_state\n",
    "        \n",
    "    def move(self, state_prime, reward):\n",
    "        \"\"\"\n",
    "        @summary: Moves to the given state with given reward and returns action\n",
    "        @param state_prime: The new state\n",
    "        @param reward: The reward\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        state = self.state\n",
    "        action = self.action\n",
    "        qtable = self.qtable\n",
    "        action_prime = -1\n",
    "\n",
    "        greedy_action = self.qtable[state_prime].argsort()[-1]\n",
    "\n",
    "        \n",
    "        choose_random_action = (1 - self.random_action_rate) <= np.random.uniform(0, 1)\n",
    "\n",
    "        if choose_random_action:\n",
    "            action_prime = np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            action_prime = greedy_action\n",
    "\n",
    "        if self.num_iteration > self.warm_up_period: # warm up period is over\n",
    "            self.random_action_rate *= self.random_action_decay_rate\n",
    "\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * qtable[state_prime, greedy_action] - qtable[state, action])\n",
    "\n",
    "        self.state = state_prime\n",
    "        self.action = action_prime    \n",
    "        self.qtable = qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class naive_QLearner(object):\n",
    "    def __init__(self, \n",
    "                 servo_num_states, \n",
    "                 num_actions, \n",
    "                 alpha, \n",
    "                 gamma, \n",
    "                 random_action_rate,\n",
    "                 random_action_decay_rate, \n",
    "                 warm_up_period, \n",
    "                 action_penalty,\n",
    "                 negative_reward_coef,\n",
    "                 interest_zone_reward_coef,\n",
    "                 initial_state,\n",
    "                 scaling_point, \n",
    "                 scaling_factor,\n",
    "                 eligibility_lambda):\n",
    "        \n",
    "        self.servo_num_states = servo_num_states\n",
    "        self.num_states = servo_num_states**2\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.random_action_rate = random_action_rate\n",
    "        self.random_action_decay_rate = random_action_decay_rate\n",
    "        self.warm_up_period = warm_up_period\n",
    "        self.state = initial_state\n",
    "        self.action = 0\n",
    "        self.action_penalty = action_penalty\n",
    "#         self.qtable = np.random.uniform(low=-1, high=1, size=(self.num_states, self.num_actions))\n",
    "        self.qtable = np.zeros((self.num_states, self.num_actions))\n",
    "#         self.qtable = np.ones((self.num_states, self.num_actions)) * 0\n",
    "        self.num_iteration = 0\n",
    "        self.last_reward = 0\n",
    "        self.negative_reward_coef = negative_reward_coef\n",
    "        self.interest_zone_reward_coef = interest_zone_reward_coef\n",
    "        self.scaling_point = scaling_point\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.traces_table = np.zeros((self.num_states, self.num_actions))\n",
    "        self.eligibility_lambda = eligibility_lambda\n",
    "    \n",
    "    def set_initial_state(self, action):\n",
    "        \"\"\"\n",
    "        @summary: Sets the initial state and returns an action\n",
    "        @param state: The initial state\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        self.state = int(self.num_states/2)\n",
    "        self.action = action #self.qtable[state].argsort()[-1]\n",
    "        \n",
    "    def get_next_state(self):\n",
    "        \n",
    "        next_state = None\n",
    "        \n",
    "        if (self.action == 0 and action_is_allowed(self, self.state, self.action)): \n",
    "            next_state = self.state + self.servo_num_states\n",
    "        elif (self.action == 1 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - self.servo_num_states\n",
    "        elif (self.action == 2 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state + 1\n",
    "        elif (self.action == 3 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - 1 \n",
    "        else:\n",
    "            next_state = self.state;\n",
    "            \n",
    "        return next_state\n",
    "        \n",
    "    def move(self, state_prime, reward):\n",
    "        \"\"\"\n",
    "        @summary: Moves to the given state with given reward and returns action\n",
    "        @param state_prime: The new state\n",
    "        @param reward: The reward\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        eligibility_lambda = self.eligibility_lambda\n",
    "        state = self.state\n",
    "        action = self.action\n",
    "        qtable = self.qtable\n",
    "        traces_table = self.traces_table\n",
    "        action_prime = -1\n",
    "        \n",
    "            \n",
    "        greedy_action = self.qtable[state_prime].argsort()[-1]\n",
    "        \n",
    "        choose_random_action = (1 - self.random_action_rate) <= np.random.uniform(0, 1)\n",
    "\n",
    "        if choose_random_action:\n",
    "            action_prime = np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            action_prime = greedy_action\n",
    "\n",
    "        if self.num_iteration > self.warm_up_period: # warm up period is over\n",
    "            self.random_action_rate *= self.random_action_decay_rate\n",
    "\n",
    "        theta = reward + gamma * qtable[state_prime, greedy_action] - qtable[state, action]\n",
    "        traces_table[state, action] += 1 \n",
    "        \n",
    "        for s in xrange(self.num_states):\n",
    "            for a in xrange(self.num_actions):\n",
    "                qtable[s, a] = qtable[state, action] + alpha * theta * traces_table[s, a]\n",
    "                traces_table[s, a] = gamma * eligibility_lambda * traces_table[s, a]\n",
    "\n",
    "        self.state = state_prime\n",
    "        self.action = action_prime    \n",
    "        self.qtable = qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_final_reward(learner, reward_table, training_iteration):\n",
    "    \n",
    "    reward_list = []\n",
    "    \n",
    "#     learned_reward_table = np.zeros((learner.num_states, learner.num_actions))\n",
    "    \n",
    "    num_exploration_step = 200000\n",
    "    \n",
    "    verbose = True\n",
    "    \n",
    "    for iteration_step in xrange(training_iteration):\n",
    "\n",
    "#         reward = reward_table[learner.state][learner.action] - learner.action_penalty\n",
    "\n",
    "#         if learner.state % learner.servo_num_states > 2 and learner.state < 30:\n",
    "#             reward = reward * learner.interest_zone_reward_coef if reward > 0 else reward\n",
    "            \n",
    "        if iteration_step < num_exploration_step: \n",
    "            \n",
    "            reward = reward_table[learner.state][learner.action] \n",
    "#             random_noise = np.random.normal(0, np.abs(ideal_reward)*.25) if ideal_reward != 0 else 0 \n",
    "#             noisy_reward = ideal_reward + random_noise - learner.action_penalty\n",
    "\n",
    "#             if learner.state % learner.servo_num_states > 2 and learner.state < 30:\n",
    "#                 reward = noisy_reward * learner.interest_zone_reward_coef if noisy_reward > 0 else noisy_reward * learner.negative_reward_coef\n",
    "#             else:\n",
    "#                 reward = noisy_reward\n",
    "\n",
    "#             if learned_reward_table[learner.state][learner.action] != 0:\n",
    "#                 learned_reward_table[learner.state][learner.action] = .5*(learned_reward_table[learner.state][learner.action] + reward)\n",
    "#             else:\n",
    "#                 learned_reward_table[learner.state][learner.action] = reward\n",
    "            \n",
    "\n",
    "            next_state = learner.get_next_state()\n",
    "\n",
    "            learner.move(next_state, reward)\n",
    "\n",
    "            learner.num_iteration += 1 \n",
    "\n",
    "            if learner.num_iteration % learner.scaling_point == 0: learner.qtable *= learner.scaling_factor\n",
    "\n",
    "            reward_list.append(reward)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "#             if verbose:\n",
    "#                 print \"Begin simulation\"\n",
    "                \n",
    "#             verbose = False\n",
    "            \n",
    "            reward = learned_reward_table[learner.state][learner.action]\n",
    "            reward_list.append(reward)\n",
    "\n",
    "\n",
    "            next_state = learner.get_next_state()\n",
    "            learner.move(next_state, reward)\n",
    "            learner.num_iteration += 1 \n",
    "\n",
    "            if learner.num_iteration % learner.scaling_point == 0: \n",
    "                learner.qtable *= learner.scaling_factor\n",
    "        \n",
    "    return np.mean(reward_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_equivalent_reward_table(params, num_simulation):\n",
    "    \n",
    "    reward_table_list = []\n",
    "    \n",
    "    for simulation_step in xrange(num_simulation):\n",
    "    \n",
    "        learner = QLearner(servo_num_states = 7,\n",
    "                           num_actions=4,\n",
    "                           alpha=float(params['alpha']),\n",
    "                           gamma=float(params['gamma']),\n",
    "                           random_action_rate= 1, #float(params['random_action_rate']),\n",
    "                           random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                           warm_up_period= 1000, #int(params['warm_up_period']),\n",
    "                           action_penalty=50,\n",
    "                           negative_reward_coef=1, #float(params['negative_reward_coef']),\n",
    "                           interest_zone_reward_coef = 1, #float(params['interest_zone_reward_coef']),\n",
    "                           initial_state=10,\n",
    "                           scaling_point = int(params['scaling_point']),\n",
    "                           scaling_factor = float(params['scaling_factor']))\n",
    "    \n",
    "        learned_reward_table = np.zeros((learner.num_states, learner.num_actions))\n",
    "\n",
    "        for iteration_step in xrange(learner.warm_up_period): \n",
    "\n",
    "            ideal_reward = reward_table[learner.state][learner.action] \n",
    "            random_noise = np.random.normal(0, np.abs(ideal_reward)*.25) if ideal_reward != 0 else 0 \n",
    "            noisy_reward = ideal_reward + random_noise #- learner.action_penalty\n",
    "\n",
    "    #         if learner.state % learner.servo_num_states > 2 and learner.state < 30:\n",
    "    #             reward = noisy_reward * learner.interest_zone_reward_coef if noisy_reward > 0 else noisy_reward * learner.negative_reward_coef\n",
    "    #         else:\n",
    "    #             reward = noisy_reward\n",
    "\n",
    "            reward = noisy_reward\n",
    "\n",
    "            if learned_reward_table[learner.state][learner.action] != 0:\n",
    "                learned_reward_table[learner.state][learner.action] = .5*(learned_reward_table[learner.state][learner.action] + reward)\n",
    "            else:\n",
    "                learned_reward_table[learner.state][learner.action] = reward\n",
    "\n",
    "            next_state = learner.get_next_state()\n",
    "\n",
    "            learner.move(next_state, learned_reward_table[learner.state][learner.action])\n",
    "\n",
    "            learner.num_iteration += 1 \n",
    "\n",
    "            if learner.num_iteration % learner.scaling_point == 0: learner.qtable *= learner.scaling_factor\n",
    "            \n",
    "        reward_table_list.append(learned_reward_table)\n",
    "        \n",
    "\n",
    "        return reward_table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_params =  {'warm_up_period': 50, \n",
    "                'random_action_decay_rate': 0.2255776270452332, \n",
    "                'scaling_factor': 4.216451025227167, \n",
    "                'alpha': 0.939846274031598, \n",
    "                'scaling_point': 220, \n",
    "                'gamma': 0.2168574129380133, \n",
    "                'random_action_rate': 0.8730281965950317}\n",
    "\n",
    "reward_table_list = compute_equivalent_reward_table(best_params, num_simulation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials\n",
    "import datetime\n",
    "\n",
    "best_score = np.inf\n",
    "\n",
    "def score(params):\n",
    "    \n",
    "    global best_score    \n",
    "    \n",
    "    num_iteration = 2\n",
    "    \n",
    "    loss_vector = []\n",
    "    \n",
    "    for i in xrange(num_iteration):\n",
    "    \n",
    "        learner = naive_QLearner(servo_num_states = 7,\n",
    "                               num_actions=4,\n",
    "                               alpha=float(params['alpha']),\n",
    "                               gamma=float(params['gamma']),\n",
    "                               random_action_rate= float(params['random_action_rate']),\n",
    "                               random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                               warm_up_period=int(params['warm_up_period']),\n",
    "                               action_penalty=50,\n",
    "                               negative_reward_coef=1, #float(params['negative_reward_coef']),\n",
    "                               interest_zone_reward_coef = 1, #float(params['interest_zone_reward_coef']),\n",
    "                               initial_state=24,\n",
    "                               scaling_point = int(params['scaling_point']),\n",
    "                               scaling_factor = float(params['scaling_factor']),\n",
    "                               eligibility_lambda = float(params['eligibility_lambda']))\n",
    "        #learner.set_initial_state(action=0)\n",
    "\n",
    "        temp_loss = - get_final_reward(learner, reward_table, training_iteration = 25000) # negative because we want to minimize loss\n",
    "        loss_vector.append(temp_loss)\n",
    "        \n",
    "    loss = np.mean(loss_vector)\n",
    "    \n",
    "    if loss < best_score: \n",
    "        print \"Searching...\", \n",
    "        print \"New best score: {0:.6f}\".format(-loss), \n",
    "        print params \n",
    "        print datetime.datetime.now().time()\n",
    "        print\n",
    "        best_score = loss\n",
    "\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "    \n",
    "def optimize(trials):\n",
    "        \n",
    "    space = {\n",
    "             'alpha': hp.uniform('alpha', 0.1, 1),\n",
    "             'gamma': hp.uniform('gamma', 0.1, 1),\n",
    "             'random_action_rate': hp.uniform('random_action_rate', 0.8,1), \n",
    "             'random_action_decay_rate': hp.uniform('random_action_decay_rate', 0.1, 1),\n",
    "             'warm_up_period': hp.choice('warm_up_period', np.arange(50,300,25)),\n",
    "             'scaling_point': hp.choice('scaling_point', np.arange(5,300, 5)),\n",
    "             'scaling_factor': hp.uniform('scaling_factor', 1., 5.),\n",
    "             'eligibility_lambda': hp.uniform('eligibility_lambda', 0, 1)\n",
    "#              'initial_state': hp.quniform('initial_state', 0, 47, 1)\n",
    "#              'negative_reward_coef': hp.uniform('negative_reward_coef', 1, 1.5),\n",
    "#              'interest_zone_reward_coef': hp.uniform('interest_zone_reward_coef', 1, 1.5)\n",
    "            }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=150)\n",
    "\n",
    "    print '------------------------' \n",
    "    print \"Done.\"\n",
    "    print \"Best parameter setting:\", best\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching... New best score: -7470.700180 {'warm_up_period': 50, 'scaling_factor': 3.346332373509635, 'eligibility_lambda': 0.32421081007985597, 'random_action_decay_rate': 0.31441926634329626, 'random_action_rate': 0.9233598188262387, 'alpha': 0.45407786670202144, 'scaling_point': 65, 'gamma': 0.9335975674063369}\n",
      "21:48:32.988520\n",
      "\n",
      "Searching... New best score: -4551.937600 {'warm_up_period': 175, 'scaling_factor': 2.580454309437858, 'eligibility_lambda': 0.044483116839559855, 'random_action_decay_rate': 0.9534692600560306, 'random_action_rate': 0.9203631301409738, 'alpha': 0.9163655519375941, 'scaling_point': 210, 'gamma': 0.9134186918860369}\n",
      "21:48:53.107709\n",
      "\n",
      "Searching... New best score: -3460.350780 {'warm_up_period': 200, 'scaling_factor': 3.8158163450920193, 'eligibility_lambda': 0.196200306662471, 'random_action_decay_rate': 0.6041113279034883, 'random_action_rate': 0.9826860881511794, 'alpha': 0.7476330592887798, 'scaling_point': 40, 'gamma': 0.5858346688513003}\n",
      "21:49:35.622095\n",
      "\n",
      "Searching... New best score: -3046.751440 {'warm_up_period': 125, 'scaling_factor': 2.8985435437467446, 'eligibility_lambda': 0.264420117401194, 'random_action_decay_rate': 0.6499669436942097, 'random_action_rate': 0.8157525461110455, 'alpha': 0.8028961449751315, 'scaling_point': 285, 'gamma': 0.6590588786165501}\n",
      "21:49:55.204833\n",
      "\n",
      "Searching... New best score: -2276.117260 {'warm_up_period': 275, 'scaling_factor': 2.0691229926642127, 'eligibility_lambda': 0.07904385718416773, 'random_action_decay_rate': 0.4851881102483955, 'random_action_rate': 0.9829354593848421, 'alpha': 0.6693288157280856, 'scaling_point': 155, 'gamma': 0.6221738653671322}\n",
      "21:51:39.794273\n",
      "\n",
      "Searching... New best score: -131.311520 {'warm_up_period': 275, 'scaling_factor': 2.882397601338105, 'eligibility_lambda': 0.4940493349463, 'random_action_decay_rate': 0.6705018378289747, 'random_action_rate': 0.823346879228213, 'alpha': 0.39862290993482785, 'scaling_point': 45, 'gamma': 0.9083451132575862}\n",
      "21:52:17.411966\n",
      "\n",
      "Searching... New best score: -29.767100 {'warm_up_period': 250, 'scaling_factor': 2.0990275569692622, 'eligibility_lambda': 0.9265817735798947, 'random_action_decay_rate': 0.35437497399467277, 'random_action_rate': 0.9009581024420438, 'alpha': 0.98428620770521, 'scaling_point': 270, 'gamma': 0.60707818212972}\n",
      "21:52:46.056687\n",
      "\n",
      "Searching... New best score: -4.011040 {'warm_up_period': 50, 'scaling_factor': 4.607497511419128, 'eligibility_lambda': 0.7081847020002938, 'random_action_decay_rate': 0.46117159289517595, 'random_action_rate': 0.8780327136377497, 'alpha': 0.8304357819602647, 'scaling_point': 245, 'gamma': 0.9894716530370067}\n",
      "21:54:08.072568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "best_params = optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_crawling(params, reward_table, training_iteration = 4000):\n",
    "    \n",
    "    state_vector = np.zeros(49)\n",
    "    action_vector = np.zeros(4)\n",
    "    reward_vector = []\n",
    "    \n",
    "    temp_reward = []\n",
    "\n",
    "    learner = QLearner(servo_num_states = 7,\n",
    "               num_actions=4,\n",
    "               alpha=float(params['alpha']),\n",
    "               gamma=float(params['gamma']),\n",
    "               random_action_rate=float(params['random_action_rate']),\n",
    "               random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "               warm_up_period=int(params['warm_up_period']),\n",
    "               action_penalty=50,\n",
    "               negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "               interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "               initial_state=24,\n",
    "               scaling_point = int(params['scaling_point']),\n",
    "               scaling_factor = float(params['scaling_factor']))\n",
    "    \n",
    "    for iteration_step in xrange(training_iteration):\n",
    "    \n",
    "        state_vector[learner.state] += 1\n",
    "        action_vector[learner.action] += 1 \n",
    "    \n",
    "        reward = reward_table[learner.state][learner.action] - learner.action_penalty\n",
    "        temp_reward.append(reward)\n",
    "\n",
    "        next_state = learner.get_next_state()\n",
    "        learner.move(next_state, reward)\n",
    "        learner.num_iteration += 1 \n",
    "\n",
    "        if learner.num_iteration % learner.scaling_point == 0: \n",
    "            learner.qtable *= learner.scaling_factor\n",
    "\n",
    "        if iteration_step > training_iteration-30:\n",
    "            print learner.state \n",
    " \n",
    "    \n",
    "    print \"\\nReward per action: \",np.mean(temp_reward)\n",
    "\n",
    "    \n",
    "    normalize_state_vector = (state_vector/np.sum(state_vector)) * 100.\n",
    "    state_map = pd.DataFrame(normalize_state_vector.reshape(learner.servo_num_states, -1))\n",
    "    sns.heatmap(state_map, linewidths=1, annot=True)\n",
    "    plt.title('State map')\n",
    "    \n",
    "    normalize_action_vector = (action_vector/np.sum(action_vector)) * 100.\n",
    "\n",
    "best_params =  {'warm_up_period': 12.0,\n",
    " 'random_action_decay_rate': 0.15894806640577777, \n",
    "'scaling_factor': 1.1080371257490829, \n",
    "'alpha': 0.999473040776306, \n",
    "'scaling_point': 75, \n",
    "'gamma': 0.9669250552091386, \n",
    "'random_action_rate': 0.8673007680007774} \n",
    "\n",
    "test_crawling(best_params, reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_crawling_naive_Q(params, reward_table, training_iteration = 500):\n",
    "    \n",
    "    state_vector = np.zeros(49)\n",
    "    action_vector = np.zeros(4)\n",
    "    reward_vector = []\n",
    "    \n",
    "    temp_reward = []\n",
    "\n",
    "    learner = naive_QLearner(servo_num_states = 7,\n",
    "                               num_actions=4,\n",
    "                               alpha=float(params['alpha']),\n",
    "                               gamma=float(params['gamma']),\n",
    "                               random_action_rate=float(params['random_action_rate']),\n",
    "                               random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                               warm_up_period=int(params['warm_up_period']),\n",
    "                               action_penalty=50,\n",
    "                               negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "                               interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "                               initial_state=24,\n",
    "                               scaling_point = int(params['scaling_point']),\n",
    "                               scaling_factor = float(params['scaling_factor']),\n",
    "                               eligibility_lambda = .5)\n",
    "    \n",
    "#     learner = QLearner(servo_num_states = 7,\n",
    "#                num_actions=4,\n",
    "#                alpha=float(params['alpha']),\n",
    "#                gamma=float(params['gamma']),\n",
    "#                random_action_rate=float(params['random_action_rate']),\n",
    "#                random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "#                warm_up_period=int(params['warm_up_period']),\n",
    "#                action_penalty=50,\n",
    "#                negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "#                interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "#                initial_state=24,\n",
    "#                scaling_point = int(params['scaling_point']),\n",
    "#                scaling_factor = float(params['scaling_factor']))\n",
    "        \n",
    "    for iteration_step in xrange(training_iteration):\n",
    "    \n",
    "        state_vector[learner.state] += 1\n",
    "        action_vector[learner.action] += 1 \n",
    "    \n",
    "        reward = reward_table[learner.state][learner.action] - learner.action_penalty\n",
    "        temp_reward.append(reward)\n",
    "\n",
    "        next_state = learner.get_next_state()\n",
    "        learner.move(next_state, reward)\n",
    "        learner.num_iteration += 1 \n",
    "\n",
    "#         if learner.num_iteration % learner.scaling_point == 0: \n",
    "#             learner.qtable *= learner.scaling_factor\n",
    "\n",
    "        if iteration_step > training_iteration-30:\n",
    "            print learner.state \n",
    " \n",
    "    \n",
    "    print \"\\nReward per action: \",np.mean(temp_reward)\n",
    "\n",
    "    \n",
    "    normalize_state_vector = (state_vector/np.sum(state_vector)) * 100.\n",
    "    state_map = pd.DataFrame(normalize_state_vector.reshape(learner.servo_num_states, -1))\n",
    "    sns.heatmap(state_map, linewidths=1, annot=True)\n",
    "    plt.title('State map')\n",
    "    \n",
    "    normalize_action_vector = (action_vector/np.sum(action_vector)) * 100.\n",
    "\n",
    "best_params =  {'warm_up_period': 12.0,\n",
    " 'random_action_decay_rate': 0.15894806640577777, \n",
    "'scaling_factor': 1.1080371257490829, \n",
    "'alpha': 0.999473040776306, \n",
    "'scaling_point': 75, \n",
    "'gamma': 0.9669250552091386, \n",
    "'random_action_rate': 0.8673007680007774} \n",
    "\n",
    "test_crawling_naive_Q(best_params, reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crawling_simulation(params, reward_table, training_iteration = 50000, num_simulation = 1):\n",
    "    \n",
    "    state_vector = np.zeros(49)\n",
    "    action_vector = np.zeros(4)\n",
    "    reward_vector = []\n",
    "    \n",
    "    \n",
    "    for simulation in tqdm(xrange(num_simulation)):\n",
    "        \n",
    "        temp_reward = []\n",
    "\n",
    "        learner = QLearner(servo_num_states = 7,\n",
    "                   num_actions=4,\n",
    "                   alpha=float(params['alpha']),\n",
    "                   gamma=float(params['gamma']),\n",
    "                   random_action_rate=float(params['random_action_rate']),\n",
    "                   random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                   warm_up_period=int(params['warm_up_period']),\n",
    "                   action_penalty=50,\n",
    "                   negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "                   interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "                   initial_state=24,\n",
    "                   scaling_point = int(params['scaling_point']),\n",
    "                   scaling_factor = float(params['scaling_factor']))#int(params['initial_state'])) #float(params['action_penalty']))\n",
    "\n",
    "        exploration_learner = QLearner(servo_num_states = 7,\n",
    "                                       num_actions=4,\n",
    "                                       alpha=float(params['alpha']),\n",
    "                                       gamma=float(params['gamma']),\n",
    "                                       random_action_rate= 1, #float(params['random_action_rate']),\n",
    "                                       random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                                       warm_up_period= 1500, #int(params['warm_up_period']),\n",
    "                                       action_penalty=50,\n",
    "                                       negative_reward_coef=1, #float(params['negative_reward_coef']),\n",
    "                                       interest_zone_reward_coef = 1, #float(params['interest_zone_reward_coef']),\n",
    "                                       initial_state=10,\n",
    "                                       scaling_point = int(params['scaling_point']),\n",
    "                                       scaling_factor = float(params['scaling_factor']))\n",
    "        \n",
    "        \n",
    "        num_exploration_step = exploration_learner.warm_up_period\n",
    "        \n",
    "        learned_reward_table = np.zeros((learner.num_states, learner.num_actions))\n",
    "        \n",
    "        previous_reward = None\n",
    "    \n",
    "        for iteration_step in xrange(training_iteration):\n",
    "            \n",
    "    \n",
    "            if iteration_step > training_iteration-30 and simulation == num_simulation-1:\n",
    "                print learner.state \n",
    "            \n",
    "            if iteration_step < num_exploration_step:\n",
    "            \n",
    "                ideal_reward = reward_table[exploration_learner.state][exploration_learner.action]                 \n",
    "                random_noise = np.random.normal(0, np.abs(ideal_reward)*.25) if ideal_reward != 0 else 0 \n",
    "                noisy_reward = ideal_reward + random_noise - exploration_learner.action_penalty\n",
    "                    \n",
    "                reward = noisy_reward\n",
    "            \n",
    "                if learned_reward_table[exploration_learner.state][exploration_learner.action] != 0:\n",
    "                    learned_reward_table[exploration_learner.state][exploration_learner.action] = .5*(learned_reward_table[exploration_learner.state][exploration_learner.action] + reward)\n",
    "                else:\n",
    "                    learned_reward_table[exploration_learner.state][exploration_learner.action] = reward\n",
    "                \n",
    "                \n",
    "                #temp_reward.append(learned_reward_table[learner.state][learner.action])\n",
    "\n",
    "                next_state = exploration_learner.get_next_state()\n",
    "            \n",
    "                exploration_learner.move(next_state, learned_reward_table[exploration_learner.state][exploration_learner.action])\n",
    "\n",
    "                exploration_learner.num_iteration += 1 \n",
    "\n",
    "                if exploration_learner.num_iteration % exploration_learner.scaling_point == 0: \n",
    "                    exploration_learner.qtable *= exploration_learner.scaling_factor\n",
    "\n",
    "            else:\n",
    "                \n",
    "                state_vector[learner.state] += 1\n",
    "                action_vector[learner.action] += 1     \n",
    "                    \n",
    "                reward = learned_reward_table[learner.state][learner.action] - learner.action_penalty\n",
    "                temp_reward.append(reward)\n",
    "                \n",
    "                next_state = learner.get_next_state()\n",
    "                learner.move(next_state, reward)\n",
    "                learner.num_iteration += 1 \n",
    "\n",
    "                if learner.num_iteration % learner.scaling_point == 0: \n",
    "                    learner.qtable *= learner.scaling_factor\n",
    "                \n",
    "                    \n",
    "        reward_vector.append(np.mean(temp_reward))\n",
    "            \n",
    "        \n",
    "    normalize_state_vector = (state_vector/np.sum(state_vector)) * 100.\n",
    "    state_map = pd.DataFrame(normalize_state_vector.reshape(learner.servo_num_states, -1))\n",
    "    sns.heatmap(state_map, linewidths=1, annot=True)\n",
    "    plt.title('State map after {} simulations in %'.format(num_simulation))\n",
    "    \n",
    "    normalize_action_vector = (action_vector/np.sum(action_vector)) * 100.\n",
    "    \n",
    "    #print len(reward_vector)\n",
    "    print \"\\nReward per action: \",np.mean(reward_vector)\n",
    "    #print normalize_action_vector\n",
    "    \n",
    "    return learner, learned_reward_table\n",
    "\n",
    "\n",
    "\n",
    "best_params = {'warm_up_period': 96.0, \n",
    "               'scaling_factor': 1.6944560688012607, \n",
    "               'random_action_decay_rate': 0.545267375797949, \n",
    "               'random_action_rate': 0.10056735562002306, \n",
    "               'alpha': 0.6227095466939823, \n",
    "               'scaling_point': 190, \n",
    "               'gamma': 0.932459803334474}\n",
    "\n",
    "best_params =   {'warm_up_period': 90.0, \n",
    "                 'scaling_factor': 3.6044006257743333, \n",
    "                 'random_action_decay_rate': 0.824709435216204, \n",
    "                 'random_action_rate': 0.834614308862227, \n",
    "                 'alpha': 0.3092353715331193, \n",
    "                 'scaling_point': 275, \n",
    "                 'gamma': 0.37419656297319936}\n",
    "\n",
    "# best_params =  {'warm_up_period': 150, \n",
    "#                 'random_action_decay_rate': 0.3887743156769585, \n",
    "#                 'scaling_factor': 1.1566975970917208, \n",
    "#                 'alpha': 0.6187041752108351, \n",
    "#                 'scaling_point': 295, \n",
    "#                 'gamma': 0.9745242012202543, \n",
    "#                 'random_action_rate': 0.9449033859491366}\n",
    "\n",
    "# best_params =  {'warm_up_period': 50, \n",
    "#                 'random_action_decay_rate': 0.2255776270452332, \n",
    "#                 'scaling_factor': 4.216451025227167, \n",
    "#                 'alpha': 0.939846274031598, \n",
    "#                 'scaling_point': 220, \n",
    "#                 'gamma': 0.2168574129380133, \n",
    "#                 'random_action_rate': 0.8730281965950317}\n",
    "\n",
    "best_params = {'warm_up_period': 50, 'random_action_decay_rate': 0.9694900649014736, 'scaling_factor': 4.583933783369448, 'alpha': 0.1011166769702368, 'scaling_point': 75, 'gamma': 0.6864430566866447, 'random_action_rate': 0.8397212427297416}\n",
    "learner, learned_reward_table = crawling_simulation(best_params, reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learned_reward_table[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_table[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class naive_QLearner(object):\n",
    "    def __init__(self, \n",
    "                 servo_num_states, \n",
    "                 num_actions, \n",
    "                 alpha, \n",
    "                 gamma, \n",
    "                 random_action_rate,\n",
    "                 random_action_decay_rate, \n",
    "                 warm_up_period, \n",
    "                 action_penalty,\n",
    "                 negative_reward_coef,\n",
    "                 interest_zone_reward_coef,\n",
    "                 initial_state,\n",
    "                 scaling_point, \n",
    "                 scaling_factor,\n",
    "                 eligibility_lambda):\n",
    "        \n",
    "        self.servo_num_states = servo_num_states\n",
    "        self.num_states = servo_num_states**2\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.random_action_rate = random_action_rate\n",
    "        self.random_action_decay_rate = random_action_decay_rate\n",
    "        self.warm_up_period = warm_up_period\n",
    "        self.state = initial_state\n",
    "        self.action = 0\n",
    "        self.action_penalty = action_penalty\n",
    "#         self.qtable = np.random.uniform(low=-1, high=1, size=(self.num_states, self.num_actions))\n",
    "        self.qtable = np.zeros((self.num_states, self.num_actions))\n",
    "#         self.qtable = np.ones((self.num_states, self.num_actions)) * 0\n",
    "        self.num_iteration = 0\n",
    "        self.last_reward = 0\n",
    "        self.negative_reward_coef = negative_reward_coef\n",
    "        self.interest_zone_reward_coef = interest_zone_reward_coef\n",
    "        self.scaling_point = scaling_point\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.traces_table = np.zeros((self.num_states, self.num_actions))\n",
    "        self.eligibility_lambda = eligibility_lambda\n",
    "    \n",
    "    def set_initial_state(self, action):\n",
    "        \"\"\"\n",
    "        @summary: Sets the initial state and returns an action\n",
    "        @param state: The initial state\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        self.state = int(self.num_states/2)\n",
    "        self.action = action #self.qtable[state].argsort()[-1]\n",
    "        \n",
    "    def get_next_state(self):\n",
    "        \n",
    "        next_state = None\n",
    "        \n",
    "        if (self.action == 0 and action_is_allowed(self, self.state, self.action)): \n",
    "            next_state = self.state + self.servo_num_states\n",
    "        elif (self.action == 1 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - self.servo_num_states\n",
    "        elif (self.action == 2 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state + 1\n",
    "        elif (self.action == 3 and action_is_allowed(self, self.state, self.action)):\n",
    "            next_state = self.state - 1 \n",
    "        else:\n",
    "            next_state = self.state;\n",
    "            \n",
    "        return next_state\n",
    "        \n",
    "    def move(self, state_prime, reward):\n",
    "        \"\"\"\n",
    "        @summary: Moves to the given state with given reward and returns action\n",
    "        @param state_prime: The new state\n",
    "        @param reward: The reward\n",
    "        @returns: The selected action\n",
    "        \"\"\"\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        eligibility_lambda = self.eligibility_lambda\n",
    "        state = self.state\n",
    "        action = self.action\n",
    "        qtable = self.qtable\n",
    "        traces_table = self.traces_table\n",
    "        action_prime = -1\n",
    "        \n",
    "            \n",
    "        greedy_action = self.qtable[state_prime].argsort()[-1]\n",
    "        \n",
    "        choose_random_action = (1 - self.random_action_rate) <= np.random.uniform(0, 1)\n",
    "\n",
    "        if choose_random_action:\n",
    "            action_prime = np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            action_prime = greedy_action\n",
    "\n",
    "        if self.num_iteration > self.warm_up_period: # warm up period is over\n",
    "            self.random_action_rate *= self.random_action_decay_rate\n",
    "\n",
    "        theta = reward + gamma * qtable[state_prime, greedy_action] - qtable[state, action]\n",
    "        traces_table[state, action] += 1 \n",
    "        \n",
    "        for s in xrange(self.num_states):\n",
    "            for a in xrange(self.num_actions):\n",
    "                qtable[s, a] = qtable[state, action] + alpha * theta * traces_table[s, a]\n",
    "                traces_table[s, a] = gamma * eligibility_lambda * traces_table[s, a]\n",
    "\n",
    "        self.state = state_prime\n",
    "        self.action = action_prime    \n",
    "        self.qtable = qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_crawling_naive_Q(params, reward_table, training_iteration = 500):\n",
    "    \n",
    "    state_vector = np.zeros(49)\n",
    "    action_vector = np.zeros(4)\n",
    "    reward_vector = []\n",
    "    \n",
    "    temp_reward = []\n",
    "\n",
    "    learner = naive_QLearner(servo_num_states = 7,\n",
    "                               num_actions=4,\n",
    "                               alpha=float(params['alpha']),\n",
    "                               gamma=float(params['gamma']),\n",
    "                               random_action_rate=float(params['random_action_rate']),\n",
    "                               random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "                               warm_up_period=int(params['warm_up_period']),\n",
    "                               action_penalty=50,\n",
    "                               negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "                               interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "                               initial_state=24,\n",
    "                               scaling_point = int(params['scaling_point']),\n",
    "                               scaling_factor = float(params['scaling_factor']),\n",
    "                               eligibility_lambda = .5)\n",
    "    \n",
    "#     learner = QLearner(servo_num_states = 7,\n",
    "#                num_actions=4,\n",
    "#                alpha=float(params['alpha']),\n",
    "#                gamma=float(params['gamma']),\n",
    "#                random_action_rate=float(params['random_action_rate']),\n",
    "#                random_action_decay_rate=float(params['random_action_decay_rate']),\n",
    "#                warm_up_period=int(params['warm_up_period']),\n",
    "#                action_penalty=50,\n",
    "#                negative_reward_coef=1.,#float(params['negative_reward_coef']),\n",
    "#                interest_zone_reward_coef = 1, #1.32, #float(params['interest_zone_reward_coef']),\n",
    "#                initial_state=24,\n",
    "#                scaling_point = int(params['scaling_point']),\n",
    "#                scaling_factor = float(params['scaling_factor']))\n",
    "        \n",
    "    for iteration_step in xrange(training_iteration):\n",
    "    \n",
    "        state_vector[learner.state] += 1\n",
    "        action_vector[learner.action] += 1 \n",
    "    \n",
    "        reward = reward_table[learner.state][learner.action] - learner.action_penalty\n",
    "        temp_reward.append(reward)\n",
    "\n",
    "        next_state = learner.get_next_state()\n",
    "        learner.move(next_state, reward)\n",
    "        learner.num_iteration += 1 \n",
    "\n",
    "#         if learner.num_iteration % learner.scaling_point == 0: \n",
    "#             learner.qtable *= learner.scaling_factor\n",
    "\n",
    "        if iteration_step > training_iteration-30:\n",
    "            print learner.state \n",
    " \n",
    "    \n",
    "    print \"\\nReward per action: \",np.mean(temp_reward)\n",
    "\n",
    "    \n",
    "    normalize_state_vector = (state_vector/np.sum(state_vector)) * 100.\n",
    "    state_map = pd.DataFrame(normalize_state_vector.reshape(learner.servo_num_states, -1))\n",
    "    sns.heatmap(state_map, linewidths=1, annot=True)\n",
    "    plt.title('State map')\n",
    "    \n",
    "    normalize_action_vector = (action_vector/np.sum(action_vector)) * 100.\n",
    "\n",
    "best_params =  {'warm_up_period': 12.0,\n",
    " 'random_action_decay_rate': 0.15894806640577777, \n",
    "'scaling_factor': 1.1080371257490829, \n",
    "'alpha': 0.999473040776306, \n",
    "'scaling_point': 75, \n",
    "'gamma': 0.9669250552091386, \n",
    "'random_action_rate': 0.8673007680007774} \n",
    "\n",
    "test_crawling_naive_Q(best_params, reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
